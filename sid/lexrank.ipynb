{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: nltk in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (3.9.2)\n",
      "Collecting indic-nlp-library\n",
      "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting breadability>=0.1.20 (from sumy)\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.7.0 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from sumy) (2.32.3)\n",
      "Collecting pycountry>=18.2.23 (from sumy)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: click in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Collecting sphinx-argparse (from indic-nlp-library)\n",
      "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
      "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting morfessor (from indic-nlp-library)\n",
      "  Using cached Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
      "Requirement already satisfied: pandas in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from indic-nlp-library) (2.2.3)\n",
      "Requirement already satisfied: numpy in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from indic-nlp-library) (2.2.4)\n",
      "Collecting chardet (from breadability>=0.1.20->sumy)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting lxml>=2.0 (from breadability>=0.1.20->sumy)\n",
      "  Downloading lxml-6.0.2-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from requests>=2.7.0->sumy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from requests>=2.7.0->sumy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from requests>=2.7.0->sumy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from requests>=2.7.0->sumy) (2025.1.31)\n",
      "Requirement already satisfied: colorama in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from pandas->indic-nlp-library) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from pandas->indic-nlp-library) (2025.2)\n",
      "Collecting sphinx>=5.1.0 (from sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinx-8.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library)\n",
      "  Downloading docutils-0.22.2-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\n",
      "Collecting sphinxcontrib-applehelp>=1.0.7 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-devhelp>=1.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-jsmath>=1.0.1 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting sphinxcontrib-qthelp>=1.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\n",
      "Requirement already satisfied: Pygments>=2.17 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.1)\n",
      "Collecting snowballstemmer>=2.2 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading snowballstemmer-3.0.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting babel>=2.13 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting alabaster>=0.7.14 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading alabaster-1.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting roman-numerals-py>=1.0.0 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading roman_numerals_py-3.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: packaging>=23.0 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
      "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "Using cached indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 5.8/6.3 MB 29.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 24.3 MB/s eta 0:00:00\n",
      "Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
      "Using cached sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
      "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
      "   ---------------------------------------- 0.0/7.7 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 6.0/7.7 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.7/7.7 MB 23.6 MB/s eta 0:00:00\n",
      "Downloading docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "   ---------------------------------------- 0.0/587.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 587.4/587.4 kB 20.3 MB/s eta 0:00:00\n",
      "Downloading lxml-6.0.2-cp312-cp312-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.0/4.0 MB 21.8 MB/s eta 0:00:00\n",
      "Downloading sphinx-8.2.3-py3-none-any.whl (3.6 MB)\n",
      "   ---------------------------------------- 0.0/3.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.6/3.6 MB 21.2 MB/s eta 0:00:00\n",
      "Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading alabaster-1.0.0-py3-none-any.whl (13 kB)\n",
      "Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 5.5/10.2 MB 25.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.2 MB 21.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.2/10.2 MB 18.6 MB/s eta 0:00:00\n",
      "Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Downloading roman_numerals_py-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading snowballstemmer-3.0.1-py3-none-any.whl (103 kB)\n",
      "Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
      "Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
      "Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
      "Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Using cached sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
      "Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
      "Building wheels for collected packages: breadability, docopt\n",
      "  Building wheel for breadability (pyproject.toml): started\n",
      "  Building wheel for breadability (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21850 sha256=42322332238e39447f2aea0128369f0261261354bf97143afb32e72b060598e1\n",
      "  Stored in directory: c:\\users\\naren\\appdata\\local\\pip\\cache\\wheels\\32\\99\\64\\59305409cacd03aa03e7bddf31a9db34b1fa7033bd41972662\n",
      "  Building wheel for docopt (pyproject.toml): started\n",
      "  Building wheel for docopt (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13858 sha256=1db18dcc026a7bf3567af236bca98dc51357d3a7340f2ba0c73fb784232d3669\n",
      "  Stored in directory: c:\\users\\naren\\appdata\\local\\pip\\cache\\wheels\\1a\\bf\\a1\\4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "Successfully built breadability docopt\n",
      "Installing collected packages: morfessor, docopt, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, snowballstemmer, roman-numerals-py, pycountry, lxml, imagesize, docutils, chardet, babel, alabaster, sphinx, breadability, sumy, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
      "Successfully installed alabaster-1.0.0 babel-2.17.0 breadability-0.1.20 chardet-5.2.0 docopt-0.6.2 docutils-0.21.2 imagesize-1.4.1 indic-nlp-library-0.92 lxml-6.0.2 morfessor-2.0.6 pycountry-24.6.1 roman-numerals-py-3.1.0 snowballstemmer-3.0.1 sphinx-8.2.3 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0 sumy-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sumy nltk indic-nlp-library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\naren/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\naren/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install required libraries if not already installed\n",
    "# pip install sumy nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")   # NEW fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Extractive Summary (LexRank) ===\n",
      "\n",
      "Sabse pehle, main aap sabhi ka swagat karna chahunga, hamare Dakshinpashchim kshetra bikri upadhyaksh.\n",
      "Pichhli baithak ke mukhya binduon ko sankshep mein kahen.\n",
      "Baithak 11:30 baje samapt ghoshit ki gayi.\n",
      "Grameen bikri ke baare mein aap sabhi ka kya vichar hai?\n",
      "Baithak ka samapan karne se pehle, hum agali baithak ki tareekh tay karte hain.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Load Hindi text\n",
    "with open(r\"E:\\Learnings\\Masters\\MTech-Applied AI - VNIT- Intellipaat\\Sem-3\\NLP\\NLP-Project---Meeting-Transcript-Summarization-Hindi-\\naren\\meeting1-hindi-english-transliterate.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Step 2: Hindi stopwords\n",
    "english_stopwords = set([\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\",\n",
    "    \"are\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\",\n",
    "    \"both\", \"but\", \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\", \"did\", \"didn't\", \"do\", \"does\",\n",
    "    \"doesn't\", \"doing\", \"don't\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\", \"has\",\n",
    "    \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he's\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n",
    "    \"himself\", \"his\", \"how\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\",\n",
    "    \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\", \"no\",\n",
    "    \"nor\", \"not\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\",\n",
    "    \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \"she\", \"she's\", \"should\", \"shouldn't\", \"so\",\n",
    "    \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\",\n",
    "    \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\",\n",
    "    \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\",\n",
    "    \"we're\", \"we've\", \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
    "    \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"won't\", \"would\", \"wouldn't\",\n",
    "    \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "]);\n",
    "\n",
    "# Step 3: Word Tokenization + Stopword Removal\n",
    "words = word_tokenize(text)  \n",
    "filtered_words = [w for w in words if w not in english_stopwords]\n",
    "filtered_text = \" \".join(filtered_words)\n",
    "\n",
    "# Step 4: Use LexRank summarizer (works at sentence level)\n",
    "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "summarizer = LexRankSummarizer()\n",
    "\n",
    "summary_sentences = 5\n",
    "summary = summarizer(parser.document, summary_sentences)\n",
    "\n",
    "# Step 5: Print summary\n",
    "print(\"=== Extractive Summary (LexRank) ===\\n\")\n",
    "for sentence in summary:\n",
    "    print(str(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (4.56.2)\n",
      "Requirement already satisfied: torch in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: numpy in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: sentencepiece in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (0.2.1)\n",
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-win_amd64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: filelock in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading hf_xet-1.1.10-cp37-abi3-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.8/2.8 MB 16.3 MB/s eta 0:00:00\n",
      "Installing collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.1.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch numpy sentencepiece hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Learnings\\Masters\\MTech-Applied AI - VNIT- Intellipaat\\Sem-2\\ML Algorithms\\InClass Labs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = \"google/mt5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: <extra_id_0> करें\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"E:\\Learnings\\Masters\\MTech-Applied AI - VNIT- Intellipaat\\Sem-3\\NLP\\NLP-Project---Meeting-Transcript-Summarization-Hindi-\\naren/meeting1-hindi.txt\"   # <-- change to your actual filename\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    transcript = f.read()\n",
    "text = \"यहाँ आपका हिंदी में पाठ आएगा। आप इसे अपनी इच्छानुसार बदल सकते हैं।\"\n",
    "# text = transcript\n",
    "# Prepare input\n",
    "input_text = \"summarize: \" + text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate summary\n",
    "outputs = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in e:\\learnings\\masters\\mtech-applied ai - vnit- intellipaat\\sem-2\\ml algorithms\\inclass labs\\myenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 15.8 MB/s eta 0:00:00\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\naren/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\naren/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "यहाँ आपका हिंदी में पाठ आएगा। यह दूसरा वाक्य है। तीसरा वाक्य।\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# text = transcript\n",
    "# Your Hindi text\n",
    "text = \"यहाँ आपका हिंदी में पाठ आएगा। यह दूसरा वाक्य है। तीसरा वाक्य।\"\n",
    "\n",
    "# Sentence tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Vectorize sentences\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Compute sentence scores (e.g., sum of TF-IDF)\n",
    "scores = tfidf_matrix.sum(axis=1).flat\n",
    "\n",
    "# Select top N sentences\n",
    "top_n = 2\n",
    "top_sentences_indices = np.argsort(scores)[-top_n:]\n",
    "\n",
    "# Extract summary\n",
    "summary_sentences = [sentences[i] for i in sorted(top_sentences_indices)]\n",
    "print(' '.join(summary_sentences))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
